{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP12yRsaITCSPWvFJmi5Dre",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrelrassya/nlpspacybeginners/blob/main/4.spaCy%E2%80%99s_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will be learning about the various pipelines in spaCy. As we have seen, spaCy offers both heuristic (rules-based) and machine learning natural language processing solutions. These solutions are activated by pipes. In this notebook, you will learn about pipes and pipelines generally and the ones offered by spaCy specifically. In a later notebook, we will explore how you can create custom pipes and add them to a spaCy pipeline. Before we jump in, let’s import spaCy."
      ],
      "metadata": {
        "id": "e8MorvWsGANo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aHevgXrRF9zr"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](http://spacy.pythonhumanities.com/_images/sample_pipeline.png)"
      ],
      "metadata": {
        "id": "2XQaK9EuIFS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most cases, you will use an off-the-shelf spaCy model. In some cases, however, an off-the-shelf model will not fill your needs or will perform a specific task very slowly. A good example of this is sentence tokenization. Imagine if you had a document that was around 1 million sentences long. Even if you used the small English model, your model would take a long time to process those 1 million sentences and separate them. In this instance, you would want to make a blank English model and simply add the Sentencizer to it. The reason is because each pipe in a pipeline will be activated (unless specified) and that means that each pipe from Dependency Parser to named entity recognition will be performed on your data. This is a serious waste of computational resources and time. The small model may take hours to achieve this task. By creating a blank model and simply adding a Sentencizer to it, you can reduce this time to merely minutes."
      ],
      "metadata": {
        "id": "6kJQoAd47vYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")"
      ],
      "metadata": {
        "id": "iNGKGs0pGB0a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, notice that we have used spacy.blank, rather than spacy.load. When we create a blank model, we simply pass the two letter combination for a language, in this case, en for English. Now, let’s use the add_pipe() command to add a new pipe to it. We will simply add a sentencizer."
      ],
      "metadata": {
        "id": "JBct8AHM7xUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe(\"sentencizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1vUPLOPWApI",
        "outputId": "13a8156c-f140-4d9e-ff74-db641a74278a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x7a1b01bff240>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "s = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
        "soup = BeautifulSoup(s.content).text.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
        "nlp.max_length = 5278439"
      ],
      "metadata": {
        "id": "fvoX4qJHWC5W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "doc = nlp(soup)\n",
        "print (len(list(doc.sents)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDT8HbtjWEYM",
        "outputId": "163e3fa8-3490-454c-a149-d883d658481e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94134\n",
            "CPU times: user 14.9 s, sys: 239 ms, total: 15.1 s\n",
            "Wall time: 15.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference in time here is remarkable. Our text string was around 5.2 million characters. The blank model with just the Sentencizer completed its task in 7.54 seconds and found around 94k sentences. The small English model, the most efficient one offered by spaCy, did the same task in 46 minutes and 15 seconds and found around 112k sentences. The small English model, in other words, took approximately 380 times longer.\n",
        "\n",
        "Often times you need to find sentences quickly, not necessarily accurately. In these instances, it makes sense to know tricks like the one above. This notebook concludes part one of this book.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vu4DmsX27pa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp2 = spacy.load(\"en_core_web_sm\")\n",
        "nlp2.max_length = 5278439"
      ],
      "metadata": {
        "id": "AAhn2dhJWGL_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "%%time\n",
        "doc = nlp2(soup)\n",
        "print (len(list(doc.sents)))\n",
        "\n",
        "output:\n",
        "112074\n",
        "Wall time: 47min 15s\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bhd6et9AWHp5",
        "outputId": "05866cb0-b5fe-45de-8863-5065deb6515a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n%%time\\ndoc = nlp2(soup)\\nprint (len(list(doc.sents)))\\n\\noutput:\\n112074\\nWall time: 47min 15s\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "nlp2.analyze_pipes()\n",
        "\n",
        "output:\n",
        "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
        "   'requires': [],\n",
        "   'scores': [],\n",
        "   'retokenizes': False},\n",
        "  'tagger': {'assigns': ['token.tag'],\n",
        "   'requires': [],\n",
        "   'scores': ['tag_acc'],\n",
        "   'retokenizes': False},\n",
        "  'parser': {'assigns': ['token.dep',\n",
        "    'token.head',\n",
        "    'token.is_sent_start',\n",
        "    'doc.sents'],\n",
        "   'requires': [],\n",
        "   'scores': ['dep_uas',\n",
        "    'dep_las',\n",
        "    'dep_las_per_type',\n",
        "    'sents_p',\n",
        "    'sents_r',\n",
        "    'sents_f'],\n",
        "   'retokenizes': False},\n",
        "  'attribute_ruler': {'assigns': [],\n",
        "   'requires': [],\n",
        "   'scores': [],\n",
        "   'retokenizes': False},\n",
        "  'lemmatizer': {'assigns': ['token.lemma'],\n",
        "   'requires': [],\n",
        "   'scores': ['lemma_acc'],\n",
        "   'retokenizes': False},\n",
        "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
        "   'requires': [],\n",
        "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
        "   'retokenizes': False}},\n",
        " 'problems': {'tok2vec': [],\n",
        "  'tagger': [],\n",
        "  'parser': [],\n",
        "  'attribute_ruler': [],\n",
        "  'lemmatizer': [],\n",
        "  'ner': []},\n",
        " 'attrs': {'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
        "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
        "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
        "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
        "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
        "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
        "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
        "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
        "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
        "  'token.ent_type': {'assigns': ['ner'], 'requires': []}}}\n",
        "  \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "VuMBcHDcWJBR",
        "outputId": "8213efb9-868a-4859-8634-2207a10d907c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nnlp2.analyze_pipes()\\n\\noutput:\\n{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\\n   'requires': [],\\n   'scores': [],\\n   'retokenizes': False},\\n  'tagger': {'assigns': ['token.tag'],\\n   'requires': [],\\n   'scores': ['tag_acc'],\\n   'retokenizes': False},\\n  'parser': {'assigns': ['token.dep',\\n    'token.head',\\n    'token.is_sent_start',\\n    'doc.sents'],\\n   'requires': [],\\n   'scores': ['dep_uas',\\n    'dep_las',\\n    'dep_las_per_type',\\n    'sents_p',\\n    'sents_r',\\n    'sents_f'],\\n   'retokenizes': False},\\n  'attribute_ruler': {'assigns': [],\\n   'requires': [],\\n   'scores': [],\\n   'retokenizes': False},\\n  'lemmatizer': {'assigns': ['token.lemma'],\\n   'requires': [],\\n   'scores': ['lemma_acc'],\\n   'retokenizes': False},\\n  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\\n   'requires': [],\\n   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\\n   'retokenizes': False}},\\n 'problems': {'tok2vec': [],\\n  'tagger': [],\\n  'parser': [],\\n  'attribute_ruler': [],\\n  'lemmatizer': [],\\n  'ner': []},\\n 'attrs': {'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\\n  'doc.sents': {'assigns': ['parser'], 'requires': []},\\n  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\\n  'token.dep': {'assigns': ['parser'], 'requires': []},\\n  'token.tag': {'assigns': ['tagger'], 'requires': []},\\n  'doc.ents': {'assigns': ['ner'], 'requires': []},\\n  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\\n  'token.head': {'assigns': ['parser'], 'requires': []},\\n  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\\n  'token.ent_type': {'assigns': ['ner'], 'requires': []}}}\\n  \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the dictionary structure. This tells us not only what is inside the pipeline, but its order. Each key after “summary” is a pipe. The value is a dictionary. This dictionary tells us a few different things. All of these value dictionaries state: “assigns” which corresponds to a value of what that particular pipe assigns to the token and doc as it passes through the pipeline. In some cases, there will be a key of “scores” in the dictionary. This indicates how the machine learning model was evaluated. We will learn more about model evaluation in our machine learning section below.\n",
        "\n"
      ],
      "metadata": {
        "id": "XpViaotY7iYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook concludes part one of this book. It has given you an umbrella overview of spaCy. Over the next few parts of this book, we will deep dive into specific areas and use spaCy to solve general and domain-specific problems from several different areas of industry. Join me as we learn to create custom models and do custom things to leverage the full potential of the spaCy library."
      ],
      "metadata": {
        "id": "QuL-6nrj7k4P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VXqn101DWK0m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}